# Bamboo
We introduce Bamboo, a new 7B LLM that boasts exceptional sparsity while delivering performance equivalent to Mistral-7B. In this repo, we provide the details of our model.

## Models
## Performance with different sparsity
Recent studies([Zhang et al., 2024](https://arxiv.org/pdf/2402.03804.pdf)) has shown that the activation sparsity exists in LLMs by only keep the top-k activation neurons in each layer. In this subsection, we show the performance of Bamboo with different sparsity with the same method.

## CDF of neurons distribution

## Objective Performance Evaluation

## Speed Evaluation

## Limitations

## Citation
Please kindly cite using the following BibTeX:

```
@misc{bamboo,
    title={Bamboo: Harmonizing Sparsity and Performance in Large Language Models}, 
    author={Yixin Song, Haotong Xie, Zeyu Mi, Haibo Chen},
    year={2024}
}
```